"use strict";(self.webpackChunkkuzu_docs=self.webpackChunkkuzu_docs||[]).push([[8378],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>c});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),d=p(n),m=r,c=d["".concat(s,".").concat(m)]||d[m]||h[m]||i;return n?a.createElement(c,l(l({ref:t},u),{},{components:n})):a.createElement(c,l({ref:t},u))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[d]="string"==typeof e?e:r,l[1]=o;for(var p=2;p<i;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},7471:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={slug:"kuzu-0.1.0-release",authors:["team"],tags:["release"]},l="K\xf9zu 0.1.0 Release",o={permalink:"/docusaurus/blog/kuzu-0.1.0-release",source:"@site/blog/2023-11-19-kuzu-v-0.1.0.md",title:"K\xf9zu 0.1.0 Release",description:"We are very happy to release K\xf9zu 0.1.0 today! This is a major release with the following set of new features and improvements:",date:"2023-11-19T00:00:00.000Z",formattedDate:"November 19, 2023",tags:[{label:"release",permalink:"/docusaurus/blog/tags/release"}],readingTime:9.075,hasTruncateMarker:!0,authors:[{name:"K\xf9zu Team",url:"https://github.com/kuzudb/",imageURL:"https://kuzudb.com/img/blog/team.jpg",key:"team"}],frontMatter:{slug:"kuzu-0.1.0-release",authors:["team"],tags:["release"]},prevItem:{title:"RAG Using Structured Data: Overview & Important Questions",permalink:"/docusaurus/blog/llms-graphs-1"},nextItem:{title:"K\xf9zu 0.0.12 Release",permalink:"/docusaurus/blog/kuzu-0.0.12-release"}},s={authorsImageUrls:[void 0]},p=[{value:"NodeGroup-Based Storage",id:"nodegroup-based-storage",level:2},{value:"String Compression",id:"string-compression",level:3},{value:"Data Ingestion Improvements",id:"data-ingestion-improvements",level:3},{value:"New Features",id:"new-features",level:2},{value:"Direct Scans of DataFrames",id:"direct-scans-of-dataframes",level:3},{value:"Copy",id:"copy",level:3},{value:"Copy To Parquet Files",id:"copy-to-parquet-files",level:4},{value:"Copy To CSV Files",id:"copy-to-csv-files",level:4},{value:"Optional <code>column_names</code> Argument in Copy From Statements",id:"optional-column_names-argument-in-copy-from-statements",level:4},{value:"Updates",id:"updates",level:3},{value:"Detach Delete",id:"detach-delete",level:4},{value:"Return Deleted Rows",id:"return-deleted-rows",level:4},{value:"Other Changes",id:"other-changes",level:3},{value:"SQL-style Cast Function",id:"sql-style-cast-function",level:4},{value:"Recursive Relationship Node Filter",id:"recursive-relationship-node-filter",level:4},{value:"Count Subquery",id:"count-subquery",level:4},{value:"New INT128 Data Type",id:"new-int128-data-type",level:4},{value:"Development",id:"development",level:2},{value:"Nightly Build",id:"nightly-build",level:3},{value:"Reduced Binary Size",id:"reduced-binary-size",level:3},{value:"Closing Remarks",id:"closing-remarks",level:2}],u={toc:p},d="wrapper";function h(e){let{components:t,...n}=e;return(0,r.kt)(d,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"We are very happy to release K\xf9zu 0.1.0 today! This is a major release with the following set of new features and improvements:"),(0,r.kt)("h2",{id:"nodegroup-based-storage"},"NodeGroup-Based Storage"),(0,r.kt)("p",null,"With this release, we have completed the major features of our NodeGroup-base storage design,\nwhich was outlined in this ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kuzudb/kuzu/issues/1474"},"issue"),". The primary goal of this design was to have a\nstorage design that is conducive to implementing compression and zone maps optimization.\nConceptually, a NodeGroup is equivalent to a ",(0,r.kt)("a",{parentName:"p",href:"https://parquet.apache.org/docs/concepts/"},"Parquet RowGroup"),", which\nrepresents a horizontal partition of a table consisting of k many nodes (k=64x2048 for now). Each k nodes' data are\nmanaged and compressed as a unit on disk files. In release v0.0.7, we had completed the first part of this design and changed our\nnode table storage to use NodeGroups. In this release, we have completed the second part of this design and now relationship\ntables are also stored as NodeGroups. That means we now compress the relationships of k many nodes together."),(0,r.kt)("p",null,"We also stores all column data in a single file ",(0,r.kt)("inlineCode",{parentName:"p"},"data.kz")," which has significantly reduced the number of database files we now maintain."),(0,r.kt)("h3",{id:"string-compression"},"String Compression"),(0,r.kt)("p",null,'We have extended our compression to compress strings in the database using dictionary compression.\nFor each string "column chunk" (which is a partition of an entire column in a table\nstoring one NodeGroup\'s values), each string s is\nstored once in a dictionary, and for each record that has value s, we store a pointer to s.\nThis design applies when storing string properties on relationship tables.\nThis is done by using 3 column chunks in total. 2 column chunks store the dictionary as follows. One "raw strings" column chunk\nstores all the unique strings in the column chunk one after another, and another "offsets column chunk" identifies\nthe beginning indices of each string. Then, one additional "index column chunk" stores the pointers to the strings\nas indices to the "offsets" column to identify the strings.\nThe offset and index columns are bitpacked in the manner of integer columns.'),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"String Compression Benchmark")),(0,r.kt)("p",null,"Here is a micro-benchmark using the Comment table in LDBC100. To compare the compression rate of each column individually,\nwe construct a new table Tx for each string column x in the Comment table, e.g., ",(0,r.kt)("inlineCode",{parentName:"p"},"Browser Used"),". Tx consists of the\ncolumn x and a serial primary key, which allows us to avoid storing any materialized hash index. We report the size of the data.kz file\nand compare against a previous version v0.0.10 of K\xf9zu."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Column"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.0.10"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.1.0"),(0,r.kt)("th",{parentName:"tr",align:null},"Difference"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Browser Used"),(0,r.kt)("td",{parentName:"tr",align:null},"4.2 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"272 MB"),(0,r.kt)("td",{parentName:"tr",align:null},"-93.5%")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Content"),(0,r.kt)("td",{parentName:"tr",align:null},"9.7 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"7.5 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"-22.7%")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Location IP"),(0,r.kt)("td",{parentName:"tr",align:null},"5 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"1.6 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"-68.0%")))),(0,r.kt)("p",null,"We also report the entire LDBC100 database size, including all database files (data.kz, indices, metadata, catalog), of v0.1.0\nand a slightly older version v0.0.8, which included compression of nodes. So this experiment reports\nboth improvements that come from storing relationship tables in compressed form as well as\nstoring strings of both node and relationship tables in compressed form."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Database"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.0.8"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.1.0"),(0,r.kt)("th",{parentName:"tr",align:null},"Difference"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"LDBC100"),(0,r.kt)("td",{parentName:"tr",align:null},"127 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"94 GB"),(0,r.kt)("td",{parentName:"tr",align:null},"-26.0%")))),(0,r.kt)("h3",{id:"data-ingestion-improvements"},"Data Ingestion Improvements"),(0,r.kt)("p",null,"Moving our relationship table storage to a NodeGroup-based one also improved our\ndata ingestion times. The following benchmark reports the loading time of the LDBC100 ",(0,r.kt)("inlineCode",{parentName:"p"},"likesComment.csv")," relationship records.\nThe file contains 242M records and takes 13 GB in raw CSV format. Below we compare v0.1.0 against v0.0.10 using a machine with\n2 Intel Xeon Platinum 8175M CPUs, each of which has 48 physical CPU cores. We used 300 GB of the 380GB total RAM during this test."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.0.10"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.1.0"),(0,r.kt)("th",{parentName:"tr",align:null},"Difference"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"8 threads"),(0,r.kt)("td",{parentName:"tr",align:null},"266.8 s"),(0,r.kt)("td",{parentName:"tr",align:null},"229.8 s"),(0,r.kt)("td",{parentName:"tr",align:null},"-13.9%")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"4 threads"),(0,r.kt)("td",{parentName:"tr",align:null},"312.5 s"),(0,r.kt)("td",{parentName:"tr",align:null},"246.8 s"),(0,r.kt)("td",{parentName:"tr",align:null},"-21.0%")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2 threads"),(0,r.kt)("td",{parentName:"tr",align:null},"446.7 s"),(0,r.kt)("td",{parentName:"tr",align:null},"335.6 s"),(0,r.kt)("td",{parentName:"tr",align:null},"-24.8%")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1 threads"),(0,r.kt)("td",{parentName:"tr",align:null},"700.8 s"),(0,r.kt)("td",{parentName:"tr",align:null},"581.9 s"),(0,r.kt)("td",{parentName:"tr",align:null},"-17.0%")))),(0,r.kt)("h2",{id:"new-features"},"New Features"),(0,r.kt)("h3",{id:"direct-scans-of-dataframes"},"Direct Scans of DataFrames"),(0,r.kt)("p",null,"We now support scanning Pandas DataFrames directly. Consider the following ",(0,r.kt)("inlineCode",{parentName:"p"},"person")," DataFrame\nthat contains two columns, ",(0,r.kt)("inlineCode",{parentName:"p"},"id")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"height_in_cm")," (only the latter will be used in the example):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"id = np.array([0, 2, 3, 5, 7, 11, 13], dtype=np.int64)\nheight_in_cm = np.array([167, 172, 183, 199, 149, 154, 165], dtype=np.uint32)\nperson = pd.DataFrame({'id': id, 'height': height_in_cm})\n")),(0,r.kt)("p",null,"The query below finds all students who are taller than the average height of the records in the ",(0,r.kt)("inlineCode",{parentName:"p"},"person")," DataFrame:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"query = 'CALL READ_PANDAS(\"person\")\n         WITH avg(height / 2.54) as height_in_inch\n         MATCH (s:student)\n         WHERE s.height > height_in_inch\n         RETURN s'\nresults = conn.execute(query)\n")),(0,r.kt)("p",null,"Details of this feature can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/query-clauses/call#read_pandas"},"here"),"."),(0,r.kt)("h3",{id:"copy"},"Copy"),(0,r.kt)("p",null,"This release comes with several new features related to Cypher's ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," clause."),(0,r.kt)("h4",{id:"copy-to-parquet-files"},"Copy To Parquet Files"),(0,r.kt)("p",null,"Query results can now be exported to Parquet files."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'COPY ( MATCH (a:Person) RETURN a.* ) TO "person.parquet";\n')),(0,r.kt)("h4",{id:"copy-to-csv-files"},"Copy To CSV Files"),(0,r.kt)("p",null,"We added serveral configuration options when exporting to CSV files."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"COPY ( MATCH (a:Person) RETURN a.* ) TO \"person.csv\" (delim = '|', header=true);\n")),(0,r.kt)("p",null,"We also improved the performance of the CSV writer. Below is a micro benchmark of exporting the LDBC100 Comment table to CSV format."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"COPY (MATCH (p:Comment) RETURN p.*) to \u2018comment.csv\u2019;\n")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.0.10"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.1.0"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Runtime"),(0,r.kt)("td",{parentName:"tr",align:null},"1239.3s"),(0,r.kt)("td",{parentName:"tr",align:null},"104.56s")))),(0,r.kt)("h4",{id:"optional-column_names-argument-in-copy-from-statements"},"Optional ",(0,r.kt)("inlineCode",{parentName:"h4"},"column_names")," Argument in Copy From Statements"),(0,r.kt)("p",null,"Users can now load data to a subset of the columns in a table. Previously, we required that if\nusers are going to load an empty table T from a file F,\ne.g., a CSV or Parquet file, then F must contain: (1) as many columns as the columns in T; and (2) in the same order as\ntable T. Now users can optionally add a ",(0,r.kt)("inlineCode",{parentName:"p"},"column_names")," argument in ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY FROM")," statements,\nwhich relaxes both of these restrictions: (1) F can now contain a subset of the columns; and (2) in arbitrary\norder, which needs to be specified in the ",(0,r.kt)("inlineCode",{parentName:"p"},"column_names")," argument. Here is an example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'CREATE NODE TABLE Person (id INT64, name STRING, comment STRING, PRIMARY KEY(id));\nCOPY Person (name, id) FROM "person.csv";\n')),(0,r.kt)("p",null,"The code above first creates a ",(0,r.kt)("inlineCode",{parentName:"p"},"Person")," table with 3 columns, and then loads two of its columns from a file\nthat contains ",(0,r.kt)("inlineCode",{parentName:"p"},"name")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"id")," values of the columns respectively.\nThe third ",(0,r.kt)("inlineCode",{parentName:"p"},"comment")," column in the table will be set to ",(0,r.kt)("inlineCode",{parentName:"p"},"NULL")," for all imported records. The details\nof this feature can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/copy"},"here"),"."),(0,r.kt)("h3",{id:"updates"},"Updates"),(0,r.kt)("h4",{id:"detach-delete"},"Detach Delete"),(0,r.kt)("p",null,"K\xf9zu now supports Cypher's ",(0,r.kt)("a",{parentName:"p",href:"/cypher/data-manipulation-clauses/delete#detach-delete"},"DETACH DELETE")," clause,\nwhich deletes a node and all of its relationships together.\nPreviously users could only use the ",(0,r.kt)("inlineCode",{parentName:"p"},"DELETE")," command, which deleted nodes that had no relationships.\nFor example, the following query deletes a ",(0,r.kt)("inlineCode",{parentName:"p"},"User")," node with ",(0,r.kt)("inlineCode",{parentName:"p"},"name")," Adam and all of its edges."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"MATCH (u:User) WHERE u.name = 'Adam' DETACH DELETE u;\n")),(0,r.kt)("h4",{id:"return-deleted-rows"},"Return Deleted Rows"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"RETURN")," clauses can now return variable bindings that were used in the ",(0,r.kt)("inlineCode",{parentName:"p"},"DELETE")," command. For example,\nyou can return nodes that were deleted in the previous DELETE statement as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"DELETE (a:Person) RETURN a;\n")),(0,r.kt)("p",null,"Details of this feature can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/data-manipulation-clauses/read-after-update"},"here"),"."),(0,r.kt)("h3",{id:"other-changes"},"Other Changes"),(0,r.kt)("h4",{id:"sql-style-cast-function"},"SQL-style Cast Function"),(0,r.kt)("p",null,"We have implemented a SQL-style ",(0,r.kt)("inlineCode",{parentName:"p"},"cast")," function ",(0,r.kt)("inlineCode",{parentName:"p"},"cast(input, target_type)")," to cast values between different\ntypes. The cast function will convert the ",(0,r.kt)("inlineCode",{parentName:"p"},"input")," argument to the ",(0,r.kt)("inlineCode",{parentName:"p"},"target_type")," if\ncasting of the input value to the target type is defined. For example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'RETURN cast("[1,2,3]", "INT[]");\n--------------------------\n| CAST([1,2,3], INT32[]) |\n--------------------------\n| [1,2,3]                |\n--------------------------\n')),(0,r.kt)("p",null,"Along with this, we are deprecating our previous way of doing casts with separate functions, e.g., ",(0,r.kt)("inlineCode",{parentName:"p"},"STRING(1.2)")," or ",(0,r.kt)("inlineCode",{parentName:"p"},'to_int64("32")'),".\nDetails of the ",(0,r.kt)("inlineCode",{parentName:"p"},"cast")," function can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/expressions/casting"},"here"),"."),(0,r.kt)("h4",{id:"recursive-relationship-node-filter"},"Recursive Relationship Node Filter"),(0,r.kt)("p",null,"Since v0.0.5 we have supported filtering the intermediate relationships that can bind to\nrecursive relationships, based on the properties of these intermediate relationships.\nWith the current release, we now support filtering the intermediate nodes that are bound to recursive relationships.\nAs we did for filtering intermediate relationships, we adopt Memgraph's syntax for this feature as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"MATCH p = (a:User)-[:Follows*1..2 (r, n | WHERE n.age > 21)]->(b:User)\nRETURN p;\n")),(0,r.kt)("p",null,"The first variable ",(0,r.kt)("inlineCode",{parentName:"p"},"r")," that is inside the recursive relationship above binds to the intermediate relationships while\nthe second variable ",(0,r.kt)("inlineCode",{parentName:"p"},"n")," binds to the intermediate nodes. The ",(0,r.kt)("inlineCode",{parentName:"p"},"|"),"symbol can be followed with a ",(0,r.kt)("inlineCode",{parentName:"p"},"WHERE")," clause\nwhere these variables can be used to express a filtering expression. This query finds all 1 to 2-hop paths between\ntwo ",(0,r.kt)("inlineCode",{parentName:"p"},"User")," nodes where the intermediate nodes of these paths have ",(0,r.kt)("inlineCode",{parentName:"p"},"age")," properties greater than 21.\nDetails of this feature can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/query-clauses/match#filter-variable-length-relationships"},"here"),"."),(0,r.kt)("h4",{id:"count-subquery"},"Count Subquery"),(0,r.kt)("p",null,"We have added support for counting subqueries, which checks the number of matches for the given pattern in the graph.\nThe output of this counting can be bound to a variable with aliasing. For example, the following query counts the\nnumber of followers of each user in the graph."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"MATCH (a:User)\nRETURN a.name, COUNT { MATCH (a)<-[:Follows]-(b:User) } AS num_follower\nORDER BY num_follower;\n")),(0,r.kt)("p",null,"The details of count subqueries can be found ",(0,r.kt)("a",{parentName:"p",href:"/cypher/subquery#count-subquery"},"here"),"."),(0,r.kt)("h4",{id:"new-int128-data-type"},"New INT128 Data Type"),(0,r.kt)("p",null,"Finally, we now have support for 16-byte signed huge integers."),(0,r.kt)("h2",{id:"development"},"Development"),(0,r.kt)("h3",{id:"nightly-build"},"Nightly Build"),(0,r.kt)("p",null,"We have setup a nightly build pipeline for K\xf9zu users who want to access our latest feature set.\nHere is how you can use the latest nightly version of K\xf9zu:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"For the Python API, the latest nightly version can be installed with ",(0,r.kt)("inlineCode",{parentName:"li"},"pip install --pre kuzu"),"."),(0,r.kt)("li",{parentName:"ul"},"For the Node.js API, the latest nightly version can be installed with ",(0,r.kt)("inlineCode",{parentName:"li"},"npm i kuzu@next"),"."),(0,r.kt)("li",{parentName:"ul"},"For the Rust API, the latest nightly version can be found at ",(0,r.kt)("a",{parentName:"li",href:"https://crates.io/crates/kuzu/versions"},"crates.io"),"."),(0,r.kt)("li",{parentName:"ul"},"For the CLI, C and C++ shared library, and the Java JAR, the latest nightly version can be downloaded from the latest run of ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/kuzudb/kuzu/actions/workflows/build-and-deploy.yml"},"this GitHub Actions pipeline"),".")),(0,r.kt)("h3",{id:"reduced-binary-size"},"Reduced Binary Size"),(0,r.kt)("p",null,"With this release, we removed our Apache Arrow dependency, which significantly reduces oure binary size.\nAdditionally, we now strip the shared library and CLI binaries of the symbols that are not needed by our\nclient APIs. This further reduces our binary sizes.\nFor example, on a MacOS arm64 platform, these two improvements achieve the following cumulative binary size reductions:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null}),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.0.10"),(0,r.kt)("th",{parentName:"tr",align:null},"Version 0.1.0"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Binary Size"),(0,r.kt)("td",{parentName:"tr",align:null},"27.2 MB"),(0,r.kt)("td",{parentName:"tr",align:null},"10.3 MB")))),(0,r.kt)("p",null,"Stripping of our other libraries (e.g. Python) is a work in progress."),(0,r.kt)("h2",{id:"closing-remarks"},"Closing Remarks"),(0,r.kt)("p",null,"As usual, we would like to thank everyone in the K\xf9zu engineering team, especially our interns, for making this release possible.\nWe look forward to your feedback!"),(0,r.kt)("p",null,"Enjoy K\xf9zu v 0.1.0 and the upcoming holiday season, which in this part of the world \ud83c\udde8\ud83c\udde6\ud83c\udde8\ud83c\udde6 coincides with\ncoming of the cold and cozy winter \ud83e\udd17\ud83e\udd17."))}h.isMDXComponent=!0}}]);