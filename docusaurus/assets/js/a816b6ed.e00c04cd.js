"use strict";(self.webpackChunkkuzu_docs=self.webpackChunkkuzu_docs||[]).push([[9089],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>m});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},h="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),h=c(a),u=o,m=h["".concat(l,".").concat(u)]||h[u]||p[u]||i;return a?n.createElement(m,s(s({ref:t},d),{},{components:a})):n.createElement(m,s({ref:t},d))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,s=new Array(i);s[0]=u;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r[h]="string"==typeof e?e:o,s[1]=r;for(var c=2;c<i;c++)s[c]=a[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},5855:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>f,contentTitle:()=>u,default:()=>b,frontMatter:()=>p,metadata:()=>m,toc:()=>g});var n=a(7462),o=(a(7294),a(3905)),i=a(23),s=a(5411),r=a(1509),l=a(6857),c=a(172),d=a(7131),h=a(5418);const p={slug:"factorization",authors:["semih"],tags:["internals"]},u="Factorization & Great Ideas from Database Theory",m={permalink:"/docusaurus/blog/factorization",source:"@site/blog/2023-01-20-factorization/index.md",title:"Factorization & Great Ideas from Database Theory",description:"Many of the core principles of how to develop DBMSs are well understood.",date:"2023-01-20T00:00:00.000Z",formattedDate:"January 20, 2023",tags:[{label:"internals",permalink:"/docusaurus/blog/tags/internals"}],readingTime:22.71,hasTruncateMarker:!0,authors:[{name:"Semih Saliho\u011flu",url:"https://cs.uwaterloo.ca/~ssalihog/",imageURL:"https://kuzudb.com/img/blog/semih.jpg",key:"semih"}],frontMatter:{slug:"factorization",authors:["semih"],tags:["internals"]},prevItem:{title:"K\xf9zu 0.0.2 Release",permalink:"/docusaurus/blog/kuzu-0.0.2-release"},nextItem:{title:"What Every Competent GDBMS Should Do (aka The Goals & Vision of K\xf9zu",permalink:"/docusaurus/blog/what-every-gdbms-should-do-and-vision"}},f={authorsImageUrls:[void 0]},g=[{value:"A Quick Background: Traditional Query Processing Using Flat Tuples",id:"a-quick-background-traditional-query-processing-using-flat-tuples",level:2},{value:"Factorization In a Nutshell",id:"factorization-in-a-nutshell",level:2},{value:"Examples When Factorization Significantly Benefits:",id:"examples-when-factorization-significantly-benefits",level:2},{value:"Less Data Copies/Movement",id:"less-data-copiesmovement",level:3},{value:"Fewer Predicate and Expression Evaluations",id:"fewer-predicate-and-expression-evaluations",level:3},{value:"Aggregations",id:"aggregations",level:3},{value:"How Does K\xf9zu Perform Factorized Query Processing?",id:"how-does-k\xf9zu-perform-factorized-query-processing",level:2},{value:"1. Factorization",id:"1-factorization",level:3},{value:"2. Ensuring Sequential Scans",id:"2-ensuring-sequential-scans",level:3},{value:"3. Avoiding Full Scans of Database Files",id:"3-avoiding-full-scans-of-database-files",level:3},{value:"A Simple Simulation",id:"a-simple-simulation",level:3},{value:"Example Experiment",id:"example-experiment",level:3},{value:"Final marks:",id:"final-marks",level:2}],y={toc:g},w="wrapper";function b(e){let{components:t,...a}=e;return(0,o.kt)(w,(0,n.Z)({},y,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Many of the core principles of how to develop DBMSs are well understood.\nFor example, a very good query compilation paradigm is to\nmap high-level queries to a logical plan of relational operators, then optimize this plan,\nand then further map it to an executable code often in the form of a physical query plan.\nSimilarly, if you want updates to a DBMS to be atomic and durable,\na good paradigm is to use a write-ahead log that serves as a source of truth\nand can be used to undo or redo operations. Many systems adopt such common wisdom paradigms.\nAs core DBMS researcher, once in a while however, you run into a very simple idea\nthat deviates from the norm that gets you very excited.\nToday, I want to write about one such idea called ",(0,o.kt)("a",{parentName:"p",href:"https://www.cs.ox.ac.uk/dan.olteanu/papers/os-sigrec16.pdf"},"factorization"),". "),(0,o.kt)("admonition",{title:"Tldr: The key takeaways are:",type:"tip"},(0,o.kt)("ul",{parentName:"admonition"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Overview of Factorization & Why Every GDBMS Must Adopt It"),": Factorization\nis a compression technique to compress the intermediate results\nthat query processors generate when evaluating many-to-many (m-n) joins.\nFactorization can compress an intermediate result size exponentially\nin the number m-n joins in the query."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Example Benefits of Factorization"),": Benefits of keeping intermediate\nresults smaller reduces the computation processors perform\non many queries. Examples include reducing copies by keeping the output\ndata size small, reducing filter and expression evaluation computations exponentially,\nand performing very fast aggregations."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"How K\xf9zu Implements Factorization:")," K\xf9zu's query processor\nis designed to achieve 3 design goals: (i) factorize intermediate results;\n(ii) always perform sequential scans of database files; and (iii) avoid\nscanning large chunks of database files when possible. In addition, the processor is\nvectorized as in modern columnar DBMSs. These design goals are achieved by passing\nmultiple ",(0,o.kt)("em",{parentName:"li"},"factorized vectors")," between each other and using modified HashJoin operators\nthat do ",(0,o.kt)("em",{parentName:"li"},"sideways information passing")," to avoid scans of entire files."))),(0,o.kt)("p",null,"This is a quite technical and long blog post and will appeal more to people who are interested\nin internals of DBMSs. It's about a technique that's quite dear to my heart called factorization,\nwhich is a very\nsimple data compression technique. Probably all\ncompression techniques you know are designed to compress database files that\nare stored on disk. Think of run-length encoding, dictionary compression, or bitpacking.\nIn contrast, you can't use factorization to compress your raw database files.\nFactorization has a very unique property:\nit is designed to compress the intermediate\ndata that are generated when query processors of DBMSs evaluate\nmany-to-many (m-n) growing joins. If you have read ",(0,o.kt)("a",{parentName:"p",href:"/docusaurus/blog/what-every-gdbms-should-do-and-vision"},"my previous blog"),",\nefficiently handling m-n joins was one of the items on my list of properties\nthat competent GDBMSs should excel in. This is because\nthe workloads of GDBMSs commonly contain m-n joins\nacross node records. Each user in a social network or an account in a financial transaction network\nor will have thousands of connections and if you want\na GDBMS to find patterns on your graphs, you are\nasking queries with m-n joins. Factorization is directly designed\nfor these workloads and because of that every competent GDBMS must develop\na factorized query processor. In fact, if I were to try to write a new analytical RDBMS,\nI would probably also integrate factorization into it."),(0,o.kt)("p",null,"This post forms the 2nd part of my 3-part posts on the contents of our ",(0,o.kt)("a",{parentName:"p",href:"https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf"},"CIDR paper"),"\nwhere we introduced K\xf9zu. The 3rd piece will be on another technique called worst-case\noptimal join algorithms, which is also designed for a specific class of m-n joins.\nBoth in this post and the next, I have two goals. First is to try to articulate these techniques\nusing a language that is accessible to general software engineers.\nSecond, is to make people appreciate the role of\npen-and-paper theory in advancing the field of DBMSs. Both of these techniques were first\narticulated in a series of purely theoretical papers which gave excellent\npractical advice on how to improve DBMS performance.\nCredit goes to the great theoreticians who pioneered these techniques whom I will cite\nin these posts. Their work should be highly appreciated."),(0,o.kt)("h2",{id:"a-quick-background-traditional-query-processing-using-flat-tuples"},"A Quick Background: Traditional Query Processing Using Flat Tuples"),(0,o.kt)("p",null,"Here is a short background on the basics of\nquery processors before I explain factorization. If you know about\nquery plans and how to interpret them,\nyou can skip to ",(0,o.kt)("a",{parentName:"p",href:"#factorization-in-a-nutshell"},"here")," after reading\nmy running example.\nConsider a database of Account node and Transfer edge records below.\nThe two Accounts with ",(0,o.kt)("inlineCode",{parentName:"p"},"accID")," fields L1 and L2 are owned by Liz and\neach have 100 incoming and 100 outgoing Transfer edges."),(0,o.kt)("div",{class:"img-center"},(0,o.kt)("img",{src:i.Z,width:"600"})),(0,o.kt)("p",null,"Now consider a 2-hop path query in Cypher returning the accID's of source\nand destinations of money flows Liz's accounts are facilitating:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[t1:Transfer]->(b:Account)-[t2:Transfer]->(c:Account)\nWHERE b.name = 'Liz' \nRETURN a.accID, c.accID\n")),(0,o.kt)("p",null,"Here's the SQL version of the query if you modeled your records as relations.\nSame query different syntax:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"SELECT a.accID, c.accID\nFROM Account a, Transfer t1, Account b, Transfer t2, Account c\nWHERE b.name = 'Liz' AND\n      t1.src = a.accID AND t1.dst = b.accID AND\n      t2.src = b.accID AND t2.dst = c.accID\n")),(0,o.kt)("p",null,'A standard query plan for this query is shown on the left in Fig. 2.\nThe plan contains some Scan operators to scan the raw node or edge records (edges could be\nscanned from a join index) and some hash join operators to perform the joins, and\na final projection operator.\nIn some GDBMSs, you might see "linear plans" that look as in Fig. 3.'),(0,o.kt)("p",null,(0,o.kt)("span",null,(0,o.kt)("img",{src:s.Z,style:{width:"50%"}})),(0,o.kt)("span",null,(0,o.kt)("img",{src:r.Z,style:{width:"50%"}}))),(0,o.kt)("p",null,"The linear plan is from our previous GraphflowDB system. Here\nyou are seeing an operator called Extend, which joins node records with their Transfer relationships to\nread the system-level IDs of the neighbors of those node records.\nFollowing the Extend is another Join operator to join the accID properties of those neighbors\n(specifically c.accID and a.accID).\nIn Neo4j, you'll instead see an Expand(All) operator, which does the Extend+Join\nin GraphflowDB in a single operator",(0,o.kt)("sup",{parentName:"p",id:"fnref-1-10fd85"},(0,o.kt)("a",{parentName:"sup",href:"#fn-1-10fd85",className:"footnote-ref"},"1")),". For very good reasons\nwe removed these Extend/Expand type operators in K\xf9zu. I will come back to this."),(0,o.kt)("p",null,"The interpretation of plans is that tuples are flowing from the bottom to top and\neach operator will take in sets of tuples and produce sets of tuples (in a pipelined fashion).\nThe key motivation for factorization is that what flows\nbetween operators are ",(0,o.kt)("strong",{parentName:"p"},"flat tuples"),". When the joins are m-n, this\nleads to many data repetitions, which one way or another leads to repeated\ncomputation in the operators. For example,\nthe final projection operator in our example would take the table shown in Figure 4 (left)."),(0,o.kt)("div",{class:"img-center"},(0,o.kt)("img",{src:c.Z,width:"700"})),(0,o.kt)("p",null,"There are 20K tuples in the flat representation because both L1 and L2 are part of\n100 incoming x 100 outgoing=10K many 2-paths. Notice the many repetitions in this relation:\nL1, L2, or Liz values, or the values in a.accID and c.accID.\nWhat gets replicated may change across systems. Some may replicate the actual values,\nsome may replicate indices where these values are stored but overall exactly 20K\ntuples would be generated. This redundancy leads to redundant computation here and there\nduring query processing."),(0,o.kt)("h2",{id:"factorization-in-a-nutshell"},"Factorization In a Nutshell"),(0,o.kt)("p",null,"Factorization addresses exactly this problem. The core reason for the redundancy\nis this observation: ",(0,o.kt)("em",{parentName:"p"},"given a fixed b value, all a's and c's are conditionally independent"),".\nMore concretely, once b is bound to node L1, each incoming neighbor ",(0,o.kt)("inlineCode",{parentName:"p"},"a")," for L1 will join\nwith each outgoing neighbor ",(0,o.kt)("inlineCode",{parentName:"p"},"c")," of L1. If you took the first standard undergraduate course in DBMSs at a university\nand you covered the theory of normalization, this is what is\ncalled a ",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Multivalued_dependency"},"multi-valued dependency"),"\nin relations. Factorization exploits such dependencies to compress\nrelations using Cartesian products.\nAbove in Figure 4 (right),\nI'm showing the same 20K tuples in a factorized format using only 400 values\n(so 2","*","(100+100) instead of 2","*","100","*","100 values). "),(0,o.kt)("p",null,"That's it! That's the core of the idea! Now of course, this simple observation leads to a ton of\nhard and non-obvious questions that the entire theory on factorization answers. For example,\ngiven a query, what are the \"factorization structures\", i.e., the Cartesian product structures\nthat can be used to compress it? Consider a simple query that counts the number of\npaths that are slightly longer:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[:Wire]->(b:Account)-[:Deposit]>(c:Account)-[:ETransfer]->(d:Account)\nRETURN count(*)\n")),(0,o.kt)("p",null,"Should you condition on b and factor out\na's from (c, d)'s or condition on c and factor out (a, b)'s from d's?\nOr you could condition on (b, c) and factor out (a)'s from (d)'s?\nTo make a choice, a system has to reason about the number of Wire, Deposit,\nand ETransfer records in the database.\nHow much and on which queries can you benefit from factorization?\nThe theoretical questions are endless.\nThe theory of factorization develops the formal foundation so that such questions can be answered and\nprovides principled first answers to these questions.\n",(0,o.kt)("a",{parentName:"p",href:"https://www.ifi.uzh.ch/en/dast/people/Olteanu.html"},"Dan Olteanu")," and his\ncolleagues, who lead this field, recently won the ",(0,o.kt)("a",{parentName:"p",href:"https://databasetheory.org/ICDT/test-of-time"},"ICDT test of time award"),"\nfor their work on factorization. ICDT is one of the two main\nacademic venues for theoretical work on DBMSs."),(0,o.kt)("p",null,"But let's take a step back and appreciate this theory because it gives an excellent\nadvice to system developers: ",(0,o.kt)("em",{parentName:"p"},"factorize your intermediate\nresults if your queries contain many-to-many joins!"),"\nRecall that GDBMSs most commonly evaluate many-to-many joins. So hence my point that\nGDBMSs should develop factorized query processors.\nThe great thing this theory shows us is that this can all be done by static analysis of the query\nduring compilation time by only inspecting the dependencies between variables in\nthe query! I won't cover the exact rules but at least in my running example,\nI hope it's clear that because there is no predicate between a's and c's, once\nb is fixed, we can factor out a's from c's."),(0,o.kt)("h2",{id:"examples-when-factorization-significantly-benefits"},"Examples When Factorization Significantly Benefits:"),(0,o.kt)("p",null,"Factorized intermediate relations can be exponentially smaller\n(in terms of the number of joins in the query)\nthan their flat versions, which\ncan yield orders of magnitude speed ups in query performance\nfor many different reasons. I will discuss three most obvious ones."),(0,o.kt)("h3",{id:"less-data-copiesmovement"},"Less Data Copies/Movement"),(0,o.kt)("p",null,"The most obvious benefit is that factorization reduces\nthe amount of data copied between buffers used by operators\nduring processing and to final ",(0,o.kt)("inlineCode",{parentName:"p"},"QueryResult")," structure\nthat the application gets access to. For example, a very cool feature of K\xf9zu\nis that it keeps final outputs in factorized format in its ",(0,o.kt)("inlineCode",{parentName:"p"},"QueryResult")," class and\nenumerates them one by one only when the user starts calling ",(0,o.kt)("inlineCode",{parentName:"p"},"QueryResult::getNext()"),"\nto read the tuples.\nIn our running example, throughout processing K\xf9zu would do copies of\n400 data values roughly instead of 20K to produce its ",(0,o.kt)("inlineCode",{parentName:"p"},"QueryResult"),'.\nNeedless to say, I could have picked a more exaggerated query, say a "star" query\nwith 6 relationships, and arbitrarily increased the difference in the copies done\nbetween a flat vs factorized processor.'),(0,o.kt)("h3",{id:"fewer-predicate-and-expression-evaluations"},"Fewer Predicate and Expression Evaluations"),(0,o.kt)("p",null,"Factorization can also reduce the amount of predicate or expression executions the system performs.\nSuppose we modify our 2-hop query a bit and put two additional filters on the query:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[e1:Transfer]->(b:Account)-[e2:Transfer]->(c:Account)\nWHERE b.name = 'Liz' AND a.balance > b.balance AND c.balance > b.balance\nRETURN *\n")),(0,o.kt)("p",null,"I'm omitting a plan for this query but a common plan would extend the plan in Figure 2 (or 3) above\nto also scan the balance properties and to run two filter operations:\n(i) above the join that joins a's and b's,\nto run the predicate ",(0,o.kt)("inlineCode",{parentName:"p"},"a.balance > b.balance"),"; (ii) after the final join in Figure 2\nto run the predicate ",(0,o.kt)("inlineCode",{parentName:"p"},"c.balance > b.balance"),". Suppose the first filter did not eliminate any tuples.\nThen, a flat processor would evaluate 20K filter executions in the second filter.\nIn contrast, the input to the second filter operator in a factorized processor\nwould be the 2 factorized tuples\nshown in Figure 4 (right) but extended with ",(0,o.kt)("inlineCode",{parentName:"p"},"balance")," properties\non a, b, and c's. Therefore there would be only 200 filter executions: (i)\nfor the first factorized tuple, there are only\n100 comparisons to execute ",(0,o.kt)("inlineCode",{parentName:"p"},"c.balance > b.balance")," since b is matched to a single\nvalue and there are 100 c values.; (ii) similarly for the 2nd factorized tuple.\nWe can obtain similar benefits when running other expressions."),(0,o.kt)("h3",{id:"aggregations"},"Aggregations"),(0,o.kt)("p",null,"This is perhaps where factorization yields largest benefits.\nOne can perform several aggregations directly on factorized tuples using\nalgebraic properties of several aggregation functions. Let's\nfor instance modify our above query to a count(","*",") query: Find the number of 2-paths that Liz is\nfacilitating. A factorized processor can simply count that there are 100","*","100 flat tuples in the first\nfactorized tuple and similarly in the second one to compute that the answer is 20K.\nOr consider doing min/max aggregation on factorized variables:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[e1:Transfer]->(b:Account)-[e2:Transfer]->(c:Account)\nWHERE b.accID = 'L1'\nRETURN max(a.balance), min(c.balance)\n")),(0,o.kt)("p",null,"This is asking: find the 2-path money flow that Liz's L1 account facilitates from the highest\nto lowest balance accounts (and only print the balances). If a processor\nprocesses the 10K 2-paths that L1 is part of in factorized form, then\nthe processor can  compute the max and min aggregations\nwith only 100 comparisons each (instead of 10K comparisons each). "),(0,o.kt)("p",null,"In short, the benefits of factorizing intermediate results just\nreduces computation and data copies here and there in many cases.\nYou can try some of these queries on K\xf9zu and compare its performance on large\ndatasets with non-factorized systems. "),(0,o.kt)("h2",{id:"how-does-k\xf9zu-perform-factorized-query-processing"},"How Does K\xf9zu Perform Factorized Query Processing?"),(0,o.kt)("p",null,"The rest will be even more technical and forms part of the technical meat of our CIDR paper;\nso continue reading if you are interested in database implementations.\nWhen designing the query processor of K\xf9zu, we had 3 design goals: "),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Factorize intermediate growing join results. "),(0,o.kt)("li",{parentName:"ol"},"Always perform sequential scans of database files from disk."),(0,o.kt)("li",{parentName:"ol"},"When possible avoid scanning entire database files from disk.")),(0,o.kt)("p",null,"3rd design goal requires some motivation, which I will provide below. Let's go one by one."),(0,o.kt)("h3",{id:"1-factorization"},"1. Factorization"),(0,o.kt)("p",null,"K\xf9zu has a vectorized query processor, which is the common wisdom\nin analytical read-optimized systems. "),(0,o.kt)("img",{align:"left",style:{width:350,paddingRight:15},src:d.Z}),"Vectorization, in the context of DBMS query processors refers to the design where operators pass a set of tuples, 1024 or 2048, between each other during processing[^2]. Existing vectorized query processors (in fact processors of all systems I'm aware of) pass *a single vector of flat tuples*. Instead, K\xf9zu's operators pass (possibly) multiple *factorized vectors of tuples* between each other. Each vector  can either be *flat* and represent a single value or *unflat* and represent a set of values, which is marked in a field called `curIdx` associated with each vector. For example, the first 10K tuples from my running example would be represented with 3 factorized vectors as on the left and would be passed to the final projection in the query plan in Figure 2. The interpretation is this: what is passed is the Cartesian product of all sets of tuples in those vectors. Operators know during compilation time how many vector groups they will take in and how many they will output. Importantly, we still do vectorized processing, i.e., each primitive operator operates on a vector of values inside tight for loops. Credit where credit's due: this simple-to-implement design was proposed by my PhD student [Amine Mhedhbi](http://amine.io/) with some feedback from me and my ex-Master's student [Pranjal Gupta](https://www.linkedin.com/in/g31pranjal/?originalSubdomain=in) and [Xiyang Feng](https://www.linkedin.com/in/xingyang-feng-14198491/?originalSubdomain=ca), who is now a core developer of K\xf9zu. And we directly adopted it in K\xf9zu. Amine has continued doing other excellent work on factorization, which we have not yet integrated, and you will need to wait until his PhD thesis is out.",(0,o.kt)("h3",{id:"2-ensuring-sequential-scans"},"2. Ensuring Sequential Scans"),(0,o.kt)("p",null,"I already told you above that\nExtend/Expand type join operators lead to non-sequential scans of database files.\nThese operators are not robust and if you are developing a disk-based system:\nnon-sequential scans will kill you on many queries. That's a mistake. Instead,\nK\xf9zu uses (modified) HashJoins which are much more robust. HashJoins do not perform any scans\nas part of the actual join computation so if the down stream scans\nare sequential, you get sequential scans. I'll give a simulation momentarily."),(0,o.kt)("h3",{id:"3-avoiding-full-scans-of-database-files"},"3. Avoiding Full Scans of Database Files"),(0,o.kt)("p",null,"Although I don't like Extend/Expand-type join operators,\nthey have a performance advantage. Suppose you had a simple 1-hop query that only asked for\nthe names of accounts that Liz's L1 account has transfered money to:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[:Transfer]->(b:Account)\nWHERE a.accID = 'L1'\nRETURN b.name\n")),(0,o.kt)("p",null,'Suppose your database has billions of transfers but L1 has made only 3 transfers to\naccounts with system-level record/node IDs: 107, 5, and 15. Then if you had\na linear plan like I showed in Figure 3, then an Extend/Expand-type\noperator could read these system-level IDs and then only scan\nthe name properties of these 3 nodes, avoiding the full scan of the names\nof all Accounts. If your query needs to read neighborhoods of millions of nodes,\nthis type of  computation that "reads the properties of each node\'s neighbors"\nwill degrade very quickly because: (i) each neighborhood\nof each node will require reading\ndifferent parts of the disk files that store those properties; and (ii)\nthe system might repeatedly read the same properties over and over from disk,\nas nodes share neighbors.\nInstead, you want to\nread all of the properties and create a hash table and read those properties\nfrom memory.\nHowever, if your query is accessing the neighborhoods of a few nodes,\nthen avoiding the scan of entire database file is an advantage.\nIn K\xf9zu, we wanted to use HashJoins but we also wanted a mechanism to scan\nonly the necessary parts of database files. We\ndo this through a technique called ',(0,o.kt)("em",{parentName:"p"},"sideways information passing"),(0,o.kt)("sup",{parentName:"p",id:"fnref-3-10fd85"},(0,o.kt)("a",{parentName:"sup",href:"#fn-3-10fd85",className:"footnote-ref"},"3")),".\nI'll simulate this below."),(0,o.kt)("h3",{id:"a-simple-simulation"},"A Simple Simulation"),(0,o.kt)("p",null,"For simplicity, we'll work on a simpler 1-hop query, so the benefits of factorization will not\nbe impressive but it will allow me to explain an entire query processing pipeline.\nConsider this count(","*",") query that counts the number of transfers the L1 account has made:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Account)-[t1:Transfer]->(b:Account)\nWHERE a.accID = L1\nRETURN count(*)\n")),(0,o.kt)("p",null,"An annotated query plan we generate is shown below. The figure shows step by step\nthe computation that will be performed and the data that will be passed between operators.\nFor this simulation, I am assuming that the record/nodeIDs of Accounts are as in\nFigure 1a above."),(0,o.kt)("img",{align:"left",style:{width:500,paddingRight:15},src:h.Z}),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"A Scan operator will scan the accId column and find the records of\nnodes with accID=L1. There is only 1 tuple (199, Liz) that will be output."),(0,o.kt)("li",{parentName:"ol"},"This tuple will passed to HashJoin's build side, which will create a hash table from it."),(0,o.kt)("li",{parentName:"ol"},'At this point the processor knows exactly the IDs of nodes, whose Transfer edges need\nto be scanned on the probe side: only the edges of node with ID 199! This is where we\ndo sideways information passing.\nSpecifically, the HashJoin constructs and passes a "nodeID filter" (effectively a bitmap)\nto the probe side Scan operator. Here, I\'m assuming the database has 1M Accounts but as you\ncan see only the position 199 is 1 and others are 0.'),(0,o.kt)("li",{parentName:"ol"},"The probe-side Scan uses the filter to only scan\nthe edges of 199 and avoids\nscanning the entire Transfers file.\nSince K\xf9zu is a GDBMS, we store the edges of nodes (and their properties)\nin a graph-optimized format called ",(0,o.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)"},"CSR"),".\nImportantly, all of the edges of 199 are stored consecutively and we output them in factorized format as:\n","[(199) X {201, 202, ..., 300}]","."),(0,o.kt)("li",{parentName:"ol"},"Next step can be skipped in an optimized system but currently we will probe the ","[(199) X {201, 202, ..., 300}]","\ntuple in the hash table and produce ","[(199, L1) X {201, 202, ..., 300}]",", which is passed to the\nfinal aggregation operator."),(0,o.kt)("li",{parentName:"ol"},'The agggregation operator counts that there are 100 "flat" tuples in ',"[(199, L1) X {201, 202, ..., 300}]",", simply\nby inspecting the size of the 2nd vector {201, 202, ..., 300} in the tuple.")),(0,o.kt)("p",null,"As you see the processing was factorized, we only did sequential scans\nbut we also avoided scanning the entire Transfer database file, achieving our 3 design goals.\nThis is a simplifid example and there are many queries that are more complex and where we\nhave more advanced modified hash join operators. But the simulation presents all core techniques\nin the system. You can read our ",(0,o.kt)("a",{parentName:"p",href:"https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf"},"CIDR paper"),"\nif you are curious about the details!"),(0,o.kt)("h3",{id:"example-experiment"},"Example Experiment"),(0,o.kt)("p",null,"How does it all perform? Quite well! Specifically this type of processing is quite robust.\nHere's an experiment from our CIDR paper to give a sense of the behavior of\nusing modified hash joins and factorization on a micro benchmark query. This query\ndoes a 2-hop query with aggregations on every node variable. This is on\nan ",(0,o.kt)("a",{parentName:"p",href:"https://ldbcouncil.org/benchmarks/snb/"},"LDBC"),"\nsocial network benchmark (SNB) dataset at scale factor 100 (so ~100GB of database). LDBC SNB\nmodels a social network where user post comments and react to these comments. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"MATCH (a:Comment)<-[:Likes]-(b:Person)-[:Likes]->(c:Comment)\nWHERE b.ID < X\nRETURN min(a.ID), min(b.ID), min(c.ID)\n")),(0,o.kt)("p",null,"Needless to say, we are picking this as it is a simple query that can demonstrate\nthe benefits of all of the 3 techniques above. Also needless to say, we could have exaggerated\nthe benefits by picking\nlarger stars or branched tree patterns but this will do.\nIn the experiment we are changing the selectivity of the predicate on the middle node, which\nchanges the output size. What we will compare is the behavior of K\xf9zu, which integrates\nthe 3 techniques above with (i) K\xf9zu-Extend: A configuration of K\xf9zu that uses factorization but instead of\nour modified HashJoins uses an Extend-like operator;\nand (ii) ",(0,o.kt)("a",{parentName:"p",href:"https://umbra-db.com/"},"Umbra"),(0,o.kt)("sup",{parentName:"p",id:"fnref-4-10fd85"},(0,o.kt)("a",{parentName:"sup",href:"#fn-4-10fd85",className:"footnote-ref"},"4")),", which represents the\nstate of the art RDBMS. Umbra is as fast as existing RDBMSs get. It probably integrates\nevery known low-level performance technique in the field.\nUmbra however does not\ndo factorization or have a mechanism to avoid scanning entire database files, so we\nexpect it to perform poorly on the above query. "),(0,o.kt)("p",null,"Here's the performance table."),(0,o.kt)("img",{align:"right",style:{width:350,paddingRight:15},src:l.Z}),"When the selectivity is very low, Extend-like operators + factorization do quite well because they don't yet suffer much from non-sequential scans and they avoid several overheads of our modified hash joins: no hash table creation and no semijoin filter mask creation. But they are not robust and degrade quickly. We can also see that even if you're Umbra, without factorization or a mechanism to avoid scanning entire files, you will not perform very well on these queries with m-n joins (even if there is only 2 of them here). We conducted several other experiments all demonstrating the robustness and scalability of factorized processing using modified hash join operators. I won't cover them but they are all in [our CIDR paper](https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf).",(0,o.kt)("h2",{id:"final-marks"},"Final marks:"),(0,o.kt)("p",null,"I am convinced that modern GDBMSs have to be factorized systems to remain\ncompetitive in performance. If your system assumes that most joins will be growing,\nfactorization is one of a handful of modern technique for such workloads\nwhose principles are relatively well understood\nand one can actually implement in a system. I am sure different factorized query processors will\nbe proposed as more people attempt at it. I was happy to see in CIDR that at least 2 systems\ngurus told me they want to integrate factorization into their systems.\nIf someone proposes a technique that can on some queries\nlead to exponential computation reductions even in a pen-and-paper theory, it is a good sign\nthat for many queries it can make the difference between a system timing out vs providing\nan actual answer."),(0,o.kt)("p",null,"Finally  there is much more on the theory of factorization, which I did not cover. From my side,\nmost interestingly, there\nare even more compressed ways to represent the intermediate results than the\nvanilla Cartesian product scheme I covered in this post. Just to raise some curiosity, what I have\nin mind is called\n",(0,o.kt)("a",{parentName:"p",href:"https://fdbresearch.github.io/principles.html"},"d-representations")," but that will have to wait\nfor another time. For now, I invite you to check our performance out on large queries\nand let us know if we are slow on some queries! The K\xf9zu team says hi (\ud83d\udc4b \ud83d\ude4b\u200d\u2640\ufe0f \ud83d\ude4b\ud83c\udffd) and\nis at your service to fix all performance bugs as we continue implementing the system!\nMy next post will be about the novel ",(0,o.kt)("a",{parentName:"p",href:"/docusaurus/blog/wcoj"},"worst-case optimal join algorithms"),", which emerged\nfrom another theoretical insight on m-n joins! Take care until then!"),(0,o.kt)("div",{className:"footnotes"},(0,o.kt)("hr",{parentName:"div"}),(0,o.kt)("ol",{parentName:"div"},(0,o.kt)("li",{parentName:"ol",id:"fn-1-10fd85"},"If you come from a very graph-focused background and/or exposed to a ton of GDBMS marketing, you might react to my statement that what I am showing are standard plans that do joins. Maybe you expected to see graph-specific operators, such as a BFS or a DFS operator because the data is a graph. Or maybe someone even dared to tell you that GDBMSs don't do joins but they do traversals. Stuff like that. These word tricks and confusing jargon really has to stop and helps no one. If joins are in the nature of the computation  you are asking a DBMSs to do, calling it something else won't change the nature of the computation. Joins are joins. Every DBMSs needs to join their records with each other.",(0,o.kt)("a",{parentName:"li",href:"#fnref-1-10fd85",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-3-10fd85"},'Note that GDBMSs are able to avoid scans of entire files because notice that they do the join on internal record/node IDs, which mean something very specific. If a system needs to scan the name property of node with record/node ID 75, it can often arithmetically compute the disk page and offset where this is stored, because record IDs are dense, i.e., start from 0, 1, 2..., and so can serve as  pointers if the system\'s storage design exploits this. This is what I was referring to as "Predefined/pointer-based joins" in my ',(0,o.kt)("a",{parentName:"li",href:"/docusaurus/blog/what-every-gdbms-should-do-and-vision"},"previous blog post"),'. This is a good feature of GDBMSs that allows them to efficiently evaluate the joins of node records that are happening along the "predefined" edges in the database. I don\'t know of a mechanism where RDBMSs can do something similar, unless they develop a mechanism to convert value-based joins to pointer-based joins. See my student ',(0,o.kt)("a",{parentName:"li",href:"https://www.vldb.org/pvldb/vol15/p1011-jin.pdf"},"Guodong's work last year in VLDB")," of how this can be done. In K\xf9zu, our sideways information passing technique follows Guodong's design in this work.",(0,o.kt)("a",{parentName:"li",href:"#fnref-3-10fd85",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-4-10fd85"},"Umbra is being developed by ",(0,o.kt)("a",{parentName:"li",href:"https://www.professoren.tum.de/en/neumann-thomas"},"Thomas Neumann")," and his group. If Thomas's name does not ring a bell let me explain his weight in the field like this. As the joke goes, in the field of DBMSs: there are gods at the top, then there is Thomas Neumann, and then other holy people, and then we mere mortals.",(0,o.kt)("a",{parentName:"li",href:"#fnref-4-10fd85",className:"footnote-backref"},"\u21a9")))))}b.isMDXComponent=!0},23:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-hop-data-50098805289935a1f613617907dcb303.png"},6857:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-hop-factorization-experiment-20c4be9f7a0f21358cd888fd15ba38a9.png"},1509:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-hop-query-plan-extend-4e1220b8e7fda83e9f20c611b4afb028.png"},5411:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2-hop-query-plan-hash-join-1749ba183a543df750077e115557b565.png"},5418:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/factorized-execution-simulation-1b0f9917e3eead4cf85e02818fd579db.png"},7131:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/factorized-vectors-fa0c388632af28cb8b3448fcdad3f8cf.png"},172:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/flat-vs-factorized-4a4ea855b7bda3921e29e9d05a2157f8.png"}}]);