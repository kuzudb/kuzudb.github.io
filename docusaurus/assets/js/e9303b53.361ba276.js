"use strict";(self.webpackChunkkuzu_docs=self.webpackChunkkuzu_docs||[]).push([[5721],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>c});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=p(a),h=r,c=d["".concat(l,".").concat(h)]||d[h]||m[h]||o;return a?n.createElement(c,i(i({ref:t},u),{},{components:a})):n.createElement(c,i({ref:t},u))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},934:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const o={slug:"kuzu-pyg-remote-backend",authors:["chang","semih"],tags:["use-case"]},i="Scaling Pytorch Geometric GNNs With K\xf9zu",s={permalink:"/docusaurus/blog/kuzu-pyg-remote-backend",source:"@site/blog/2023-05-10-kuzu-pyg-rb.md",title:"Scaling Pytorch Geometric GNNs With K\xf9zu",description:"In this post, we'll walk through how to use K\xf9zu as a Pytorch Geometric (PyG) Remote Backend to train a GNN model on very large graphs that do not fit on your machine's RAM.",date:"2023-05-10T00:00:00.000Z",formattedDate:"May 10, 2023",tags:[{label:"use-case",permalink:"/docusaurus/blog/tags/use-case"}],readingTime:12.39,hasTruncateMarker:!0,authors:[{name:"Chang Liu",url:"https://www.linkedin.com/in/mewim/",imageURL:"https://kuzudb.com/img/blog/chang.gif",key:"chang"},{name:"Semih Saliho\u011flu",title:"CEO of K\xf9zu Inc & Associate Prof. at UWaterloo",url:"https://cs.uwaterloo.ca/~ssalihog/",imageURL:"https://kuzudb.com/img/blog/team.jpg",key:"semih"}],frontMatter:{slug:"kuzu-pyg-remote-backend",authors:["chang","semih"],tags:["use-case"]},prevItem:{title:"K\xf9zu 0.0.4 Release",permalink:"/docusaurus/blog/kuzu-0.0.4-release"},nextItem:{title:"K\xf9zu 0.0.3 Release",permalink:"/docusaurus/blog/kuzu-0.0.3-release"}},l={authorsImageUrls:[void 0,void 0]},p=[{value:"Dataset, Predictive Task, and GNN Model",id:"dataset-predictive-task-and-gnn-model",level:2},{value:"Step 1: Preliminaries and Loading ogbn-papers100M into K\xf9zu",id:"step-1-preliminaries-and-loading-ogbn-papers100m-into-k\xf9zu",level:2},{value:"Step 2: Get K\xf9zu Remote Backend by Calling <code>db.get_torch_geometric_remote_backend()</code>",id:"step-2-get-k\xf9zu-remote-backend-by-calling-dbget_torch_geometric_remote_backend",level:2},{value:"Step 3: Define &amp; Pass K\xf9zu&#39;s <code>feature_store</code> and <code>graph_store</code> to your GNN Model",id:"step-3-define--pass-k\xf9zus-feature_store-and-graph_store-to-your-gnn-model",level:2},{value:"Adjusting K\xf9zu&#39;s Buffer Pool Size",id:"adjusting-k\xf9zus-buffer-pool-size",level:3},{value:"An Experiment Demonstrating Throughput Numbers With Different Buffer Pool Sizes",id:"an-experiment-demonstrating-throughput-numbers-with-different-buffer-pool-sizes",level:2},{value:"Next Steps",id:"next-steps",level:2}],u={toc:p},d="wrapper";function m(e){let{components:t,...a}=e;return(0,r.kt)(d,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"In this post, we'll walk through how to use K\xf9zu as a ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/advanced/remote.html"},"Pytorch Geometric (PyG) ",(0,r.kt)("em",{parentName:"a"},"Remote Backend"))," to train a GNN model on very large graphs that do not fit on your machine's RAM. "),(0,r.kt)("p",null,"Let's start with a quick overview of PyG Remote Backends: PyG Remote Backends are plug-in replacements for PyG's in-memory graph and feature stores, so they can be used seamlessly with the rest of the PyG interfaces to develop your GNN models. If a PyG Remote Backend is a disk-based storage system, such as K\xf9zu, PyG will fetch subgraphs from K\xf9zu, which stores and scans its data from disk, allowing you to train models on very large graphs for which PyG's in-memory storage would run out of memory and fail."),(0,r.kt)("p",null,"As you'll see, if you already have PyG models you have developed in Python, replacing PyG's default storage with K\xf9zu is extremely simple. ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"It\nconsists of loading your graph into K\xf9zu and then changing 1 line of code in your PyG model")),". To demonstrate how simple this is and how it performs,\nse will follow this ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pyg-team/pytorch_geometric/tree/master/examples/kuzu/papers_100M"},"Sample Code")," to demonstrate how to do this.\nSo let's get to it!"),(0,r.kt)("h2",{id:"dataset-predictive-task-and-gnn-model"},"Dataset, Predictive Task, and GNN Model"),(0,r.kt)("p",null,"Let's start by describing our graph dataset, our predictive task, and the GNN model we will use for the predictive task."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Dataset"),": We will use the ",(0,r.kt)("inlineCode",{parentName:"p"},"ogbn-papers100M")," dataset of ~100M nodes and ~2.5B edges from the ",(0,r.kt)("a",{parentName:"p",href:"https://ogb.stanford.edu/"},"Open Graph Benchmark"),' (OGB). To find the dataset,\nyou can search for "ogbn-papers100M" ',(0,r.kt)("a",{parentName:"p",href:"https://ogb.stanford.edu/docs/nodeprop/"},"here"),". The dataset takes about 128GB of RAM when using PyG's default in-memory storage. The graph's nodes and edges model the following:"),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Nodes")," are papers that have these properties:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ID"),": an int64 node identifier"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"year"),": the publication date of the paper (you can ignore this as it will not be used in our example but this property is part of the dataset)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"x"),": 128-dimensional node features (so 128-size float tensors)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"y"),": a numeric label indicating the category/field of the paper. These numbers indicate different ",(0,r.kt)("a",{parentName:"li",href:"https://arxiv.org/category_taxonomy"},"arXiv categories"),' for\npapers. Although the exact mapping is not important, you can think of these for example as 0 indicating "physics", 2 indicating "geometry" etc.')),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Edges/Relationships")," are citations between papers and do not contain any properties."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Predictive task:")," Predict the ",(0,r.kt)("inlineCode",{parentName:"p"},"y")," labels of nodes using the node features stored in the ",(0,r.kt)("inlineCode",{parentName:"p"},"x")," properties."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"GNN Model"),": We will train a 3-layer GraphSage model that contains 5.6 million parameters to perform this predictive task. Our model is based on the implementation ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/mengyangniu/ogbn-papers100m-sage/tree/main"},"here"),". We picked this model because it was one of the better-performing models in the ",(0,r.kt)("a",{parentName:"p",href:"https://ogb.stanford.edu/docs/leader_nodeprop/"},"PyG Leaderboard for the ogbn-papers100M dataset"),' (search "GraphSAGE_res_incep" under "Leaderboard for ogbn-papers100M") that we could develop using pre-existing layers in the PyG library (so we do not have to write any custom layers).'),(0,r.kt)("h2",{id:"step-1-preliminaries-and-loading-ogbn-papers100m-into-k\xf9zu"},"Step 1: Preliminaries and Loading ogbn-papers100M into K\xf9zu"),(0,r.kt)("p",null,"As a preliminary, the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kuzu/papers_100M/prepare_data.py"},(0,r.kt)("inlineCode",{parentName:"a"},"prepare_data.py"))," script in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/pyg-team/pytorch_geometric/tree/master/examples/kuzu/papers_100M"},"Sample Code")," generates four numpy files for each property of the papers: (i) ",(0,r.kt)("inlineCode",{parentName:"p"},"./ids.npy"),"; (ii) ",(0,r.kt)("inlineCode",{parentName:"p"},"./node_feat.npy")," (storing ",(0,r.kt)("inlineCode",{parentName:"p"},"x")," properties); (iii) ",(0,r.kt)("inlineCode",{parentName:"p"},"./node_year.npy"),"; and (iv) ",(0,r.kt)("inlineCode",{parentName:"p"},"./node_label.npy")," (storing ",(0,r.kt)("inlineCode",{parentName:"p"},"y")," labels). In addition, it will generate an ",(0,r.kt)("inlineCode",{parentName:"p"},"./edge_index.csv")," file that stores the citation relationships. In the below code snippets, we will assume you have gone through those steps."),(0,r.kt)("p",null,"Let's start with how you load the ",(0,r.kt)("inlineCode",{parentName:"p"},"ogbn-papers100M")," dataset into K\xf9zu. You will first need to define a ",(0,r.kt)("inlineCode",{parentName:"p"},"paper")," NODE TABLE and a ",(0,r.kt)("inlineCode",{parentName:"p"},"cite")," REL TABLE, whose schemas will follow exactly the structure of the dataset and then use ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY FROM")," statements in K\xf9zu's version of Cypher to ingest those numpy and csv files into your ",(0,r.kt)("inlineCode",{parentName:"p"},"paper")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"cite")," tables:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'...\nimport kuzu\nimport numpy as np\n...\n\nprint("Creating an empty K\xf9zu database under the papers100M directory...")\ndb = kuzu.Database(\'papers100M\')\nconn = kuzu.Connection(db, num_threads=cpu_count())\nprint("Creating K\xf9zu tables...")\nconn.execute(\n    "CREATE NODE TABLE paper(id INT64, x FLOAT[128], year INT64, y FLOAT, "\n    "PRIMARY KEY (id));")\nconn.execute("CREATE REL TABLE cites (FROM paper TO paper, MANY_MANY);")\nprint("Copying nodes to K\xf9zu tables...")\nconn.execute(\'COPY paper FROM ("%s",  "%s",  "%s", "%s") BY COLUMN;\' %\n             (\'./ids.npy\', \'./node_feat.npy\', \'./node_year.npy\', \'./node_label.npy\'))\nprint("Copying edges to K\xf9zu tables...")\nconn.execute(\'COPY cites FROM "%s";\' % (\'./edge_index.csv\'))\nprint("All done!")\n')),(0,r.kt)("p",null,"The one important note here is that you should store your node features using ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/cypher/data-types/list.html"},"K\xf9zu's FIXED-LIST data type")," using ",(0,r.kt)("inlineCode",{parentName:"p"},"FLOAT[128]")," syntax (instead of the less efficient VAR-LIST data type, which uses ",(0,r.kt)("inlineCode",{parentName:"p"},"FLOAT[]")," syntax for lists that can have different lengths). FIXED-LIST is a data type that we specifically added to K\xf9zu to efficiently store node features and embeddings in graph ML applications."),(0,r.kt)("h2",{id:"step-2-get-k\xf9zu-remote-backend-by-calling-dbget_torch_geometric_remote_backend"},"Step 2: Get K\xf9zu Remote Backend by Calling ",(0,r.kt)("inlineCode",{parentName:"h2"},"db.get_torch_geometric_remote_backend()")),(0,r.kt)("p",null,"After loading your data to K\xf9zu, the only thing you have to do is to call the ",(0,r.kt)("inlineCode",{parentName:"p"},"get_torch_geometric_remote_backend()")," function on your Database object ",(0,r.kt)("inlineCode",{parentName:"p"},"db"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"feature_store, graph_store = db.get_torch_geometric_remote_backend(multiprocessing.cpu_count())\n")),(0,r.kt)("p",null,"This function returns two objects that implement PyG's Remote Backend interfaces: (i) ",(0,r.kt)("inlineCode",{parentName:"p"},"feature_store")," is an instance of ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.FeatureStore.html#torch_geometric.data.FeatureStore"},(0,r.kt)("inlineCode",{parentName:"a"},"torch_geometric.data.FeatureStore")),"; and (ii) ",(0,r.kt)("inlineCode",{parentName:"p"},"graph_store")," is an instance of ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.GraphStore.html#torch_geometric.data.GraphStore"},(0,r.kt)("inlineCode",{parentName:"a"},"torch_geometric.data.GraphStore")),". These two handles are your K\xf9zu Remote Backends that you can pass to your PyG models/subgraph samplers and they will make your existing PyG models work seamllessly with K\xf9zu! That's all\nyou really have to know about how to use K\xf9zu as a Remote Backend. ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"There is no more K\xf9zu functions you have to call in the rest of the demonstration. You only have\nto do 1 line of code change in your regular PyG code.")),"\nThe rest of the example contains standard code you normally write to develop your PyG models."),(0,r.kt)("h2",{id:"step-3-define--pass-k\xf9zus-feature_store-and-graph_store-to-your-gnn-model"},"Step 3: Define & Pass K\xf9zu's ",(0,r.kt)("inlineCode",{parentName:"h2"},"feature_store")," and ",(0,r.kt)("inlineCode",{parentName:"h2"},"graph_store")," to your GNN Model"),(0,r.kt)("p",null,"First, we'll define the GraphSage model in PyG. We'll put ",(0,r.kt)("inlineCode",{parentName:"p"},"..."),"'s here and there to shorten the example because, as we said above, this is your regular PyG code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# Define the model for training. The model is ported from\n# https://github.com/mengyangniu/ogbn-papers100m-sage\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation,\n                 dropout):\n        super().__init__()\n        self.n_layers = n_layers\n        ...\n\n    def forward(self, edge_list, x):\n        ...\n        for layer_index, layer in enumerate(self.layers):\n            ....\n        return self.mlp(collect)\n")),(0,r.kt)("p",null,"Next, we will enable PyG to use K\xf9zu's Remote Backend when training. We create a ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/loader/neighbor_loader.html"},(0,r.kt)("inlineCode",{parentName:"a"},"torch_geometric.loader.NeighborLoader")),", which is the subgraph sampler we will use, and pass the ",(0,r.kt)("inlineCode",{parentName:"p"},"feature_store")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"graph_store")," we obtained from K\xf9zu to it. ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("em",{parentName:"strong"},"This is the 1 line change you have to do!"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# Plug the graph store and feature store into the NeighborLoader\nkuzu_sampler = NeighborLoader(\n    data=(feature_store, graph_store),\n    num_neighbors={('paper', 'cites', 'paper'): [12, 12, 12]},\n    batch_size=LOADER_BATCH_SIZE,\n    input_nodes=('paper', input_nodes),\n    num_workers=4,\n    filter_per_worker=False,\n)\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"data=(feature_store, graph_store)"))," is the important line. When you use this sampler in training to construct mini-batches, it will perform subgraph sampling and load the required node features from K\xf9zu automatically and return a ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.HeteroData.html"},(0,r.kt)("inlineCode",{parentName:"a"},"torch_geometric.data.HeteroData"))," object, which can be directly plugged into a GNN model. That training code looks like this (again abbreviated because this is all PyG code):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"model = SAGE(128, 1024, 172, 3, torch.nn.functional.relu, 0.2)\n...\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\ncriterion = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(NUM_EPOCHS):\n    i = 0\n    start_time = time.time()\n    // **The below for loop line is where we ask the sampler to\n    // sample a mini batch\n    for b in kuzu_sampler:\n        x = b['paper']['x']\n        y = b['paper']['y']\n        edge_index = b['paper', 'cites', 'paper'].edge_index\n        ...\n        model.train()\n        optimizer.zero_grad()\n        out = model(edge_index, x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        ...\n        i += 1\n")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"for b in kuzu_sampler:")," is the exact line where the sampler will end up calling on K\xf9zu to sample a subgraph and scan the features of the nodes in that subgraph. This all ends up using K\xf9zu's disk-based storage, allowing you to train GNNs on graphs that don't fit on your RAM. One distinct advantage of K\xf9zu is that, because it is an embeddable DBMS,\nwe can do the conversion of scanned node features from K\xf9zu into PyG's tensors as a zero-copy operation. We simply write the scanned node features into a buffer array allocated in Python without any additional data transfer between the systems."),(0,r.kt)("p",null,"Currently, only the ",(0,r.kt)("inlineCode",{parentName:"p"},"feature_store")," scans data from K\xf9zu's disk-based storage. For ",(0,r.kt)("inlineCode",{parentName:"p"},"graph_store"),", our current implementation stores the entire graph topology in COO format in memory. This does limit how much you can scale, but in many models trained on large graphs, features take up more space than the graph topology, so scaling node features out of memory should still allow you to scale to very lage graphs that won't fit in your RAM."),(0,r.kt)("h3",{id:"adjusting-k\xf9zus-buffer-pool-size"},"Adjusting K\xf9zu's Buffer Pool Size"),(0,r.kt)("p",null,"As with most DBMSs, K\xf9zu has a Buffer Manager that maintains a buffer pool to keep parts of the database in memory. When you use K\xf9zu, you decide how much memory to allocate to it. The more memory you give to K\xf9zu, the less I/O it will perform on scans. So, in the context of this post, the larger the buffer manager size you set, the faster your training time will be when training large graphs out of memory. You set K\xf9zu's buffer pool size when you construct your ",(0,r.kt)("inlineCode",{parentName:"p"},"Database")," object, before you call the ",(0,r.kt)("inlineCode",{parentName:"p"},"get_torch_geometric_remote_backend()")," function. For example, the code below sets the BM size to ",(0,r.kt)("inlineCode",{parentName:"p"},"40 * 1024**3")," bytes, which is equal to 40GB. You should set it as high as possible without running out of memory for performance reasons."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'KUZU_BM_SIZE = 40 * 1024**3\n# Create kuzu database\ndb = kuzu.Database("papers100M", KUZU_BM_SIZE)\nfeature_store, graph_store = db.get_torch_geometric_remote_backend(\n    mp.cpu_count())\n')),(0,r.kt)("h2",{id:"an-experiment-demonstrating-throughput-numbers-with-different-buffer-pool-sizes"},"An Experiment Demonstrating Throughput Numbers With Different Buffer Pool Sizes"),(0,r.kt)("p",null,"Let's demonstrate what troughput numbers you can expect under different memory settings.\nAs a baseline we will first measure the throughput of training\nas time/batch using PyG's default in-memory\nstorage. This seting uses ~106GB of memory.\nWe will then simulate limited memory settings by training the same\nmodel using K\xf9zu Remote Backend and limiting K\xf9zu's buffer pool size to\ndifferent levels.\nHere are the important configurations for the experiment:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Available RAM in the machine: 384GB RAM"),(0,r.kt)("li",{parentName:"ul"},"CPU: Two Xeon Platinum 8175M (48 cores/96 threads)"),(0,r.kt)("li",{parentName:"ul"},"GPU: RTX 4090 with 24GB GPU memory"),(0,r.kt)("li",{parentName:"ul"},"SSD in the system for disk storage: 2TB Kingston KC3000 NVMe SSD"),(0,r.kt)("li",{parentName:"ul"},"Mini-batch size: 1152. Recall the ",(0,r.kt)("inlineCode",{parentName:"li"},"kuzu_sampler = NeighborLoader(...)")," that we defined above. There we gave this argument\n",(0,r.kt)("inlineCode",{parentName:"li"},"num_neighbors={('paper', 'cites', 'paper'): [12, 12, 12]}")," to the ",(0,r.kt)("inlineCode",{parentName:"li"},"NeighborLoader"),", which means that the sampler will sample 3-degree neighbors of these 1152 nodes,\nsampling 12 neighbors at each degree.\nWe picked 1152 as our mini-batch size because this is the size at which we generate batches that take a peak of 23GB of memory, so beyond this we would run out of GPU memory. ",(0,r.kt)("sup",{parentName:"li",id:"fnref-1-5f0073"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1-5f0073",className:"footnote-ref"},"1"))),(0,r.kt)("li",{parentName:"ul"},"#"," PyG Workers: 16 (we did a parameter sweep and setting this to 4, 8, 16 perform very similarly)"),(0,r.kt)("li",{parentName:"ul"},"#"," K\xf9zu Query Processor Threads: 24 (48 and 96 also perform similarly)")),(0,r.kt)("p",null,"We will run K\xf9zu with 60GB, 40GB, 20GB, and 10GB buffer pool size.\nThe lower K\xf9zu's buffer pool size, the more\ndisk I/Os K\xf9zu will perform. Note however that in this experiment K\xf9zu will use more memory than\nthese sizes for two reasons: (i) K\xf9zu stores some parts of the database always in memory\nthough this is not very important in this setting; (ii) As we said, currently\nK\xf9zu Remote Backend uses in-memory storage for the graph topology (but not node features!),\nwhich takes ~48GB of RAM. So you can roughly think of K\xf9zu using 48 + BM size in these experiments."),(0,r.kt)("p",null,"We will do 500 batches of training and report the throughput number as average end-to-end time/batch.\nWe also report the time that's spent on GPU for Training as ",(0,r.kt)("inlineCode",{parentName:"p"},"Training Time (s)")," and\ntime spent on copying data from CPU to GPU as ",(0,r.kt)("inlineCode",{parentName:"p"},"CPU-to-GPU Copying Time (s)"),". For\nK\xf9zu configurations, you can roughly\ninterpret ",(0,r.kt)("inlineCode",{parentName:"p"},"Per Batch Time (s) -  Training Time (s) -  CPU-to-GPU Copying Time (s)"),"\nas the time spent for scanning data from K\xf9zu into CPU's memory. We expect that to increase\nas we lower the BM size."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Configuration"),(0,r.kt)("th",{parentName:"tr",align:null},"Per Batch Time (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"Training Time (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"CPU-to-GPU Copying Time"),(0,r.kt)("th",{parentName:"tr",align:null},"Time Scanning Data from K\xf9zu"),(0,r.kt)("th",{parentName:"tr",align:null},"Memory Usage"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"PyG In-memory"),(0,r.kt)("td",{parentName:"tr",align:null},"0.281"),(0,r.kt)("td",{parentName:"tr",align:null},"0.240"),(0,r.kt)("td",{parentName:"tr",align:null},"0.024"),(0,r.kt)("td",{parentName:"tr",align:null},"---"),(0,r.kt)("td",{parentName:"tr",align:null},"~110 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=60GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.380 (1.35x)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.239"),(0,r.kt)("td",{parentName:"tr",align:null},"0.018"),(0,r.kt)("td",{parentName:"tr",align:null},"0.123"),(0,r.kt)("td",{parentName:"tr",align:null},"~110 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=40GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.513 (1.82x)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.239"),(0,r.kt)("td",{parentName:"tr",align:null},"0.022"),(0,r.kt)("td",{parentName:"tr",align:null},"0.251"),(0,r.kt)("td",{parentName:"tr",align:null},"~90 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=20GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"1.162 (4.88x)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.238"),(0,r.kt)("td",{parentName:"tr",align:null},"0.022"),(0,r.kt)("td",{parentName:"tr",align:null},"0.901"),(0,r.kt)("td",{parentName:"tr",align:null},"~70 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=10GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"1.190 (4.23x)"),(0,r.kt)("td",{parentName:"tr",align:null},"0.238"),(0,r.kt)("td",{parentName:"tr",align:null},"0.022"),(0,r.kt)("td",{parentName:"tr",align:null},"0.930"),(0,r.kt)("td",{parentName:"tr",align:null},"~60 GB")))),(0,r.kt)("p",null,"So, when have enough memory, there is about 1.35x slow down (from 0.281s to 0.380s per batch)\ncompared to using PyG's default storage. This\nis the case when K\xf9zu has enough buffer memory (60GB) to store the features but we still incur the cost of\nscanning them through K\xf9zu's buffer manager. So no disk I/O happens (except the first time\nthe features are scanned to the buffer manager). When we use 40GB of buffer pool and below, we start doing some I/O,\nand the average time per batch degrade to 0.513, 1.162, amd 1.190 respectively when using 40GB, 20GB, and 10GB.\nWe seem to stabilize around 4x degradation at 10GB or 20GB level, where most of the feature scans\nare now happening from disk. These numbers hopefully look good for many settings!"),(0,r.kt)("h2",{id:"next-steps"},"Next Steps"),(0,r.kt)("p",null,"We will be doing 2 immediate optimizations in the next few releases\nrelated to K\xf9zu's PyG integration.\nFirst, we will change our ",(0,r.kt)("inlineCode",{parentName:"p"},"graph_store")," to use an in DBMS subgraph sampler, so we can virtually work at any limited memory level.\nSecond, in an even earlier release, we had a more basic PyG integration feature, the\n",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/client-apis/python-api/query-result.html#query_result.QueryResult.get_as_torch_geometric"},(0,r.kt)("inlineCode",{parentName:"a"},"QueryResult.get_as_torch_geometric()"))," function.\nThis feature is more of an ETL feature. It is designed for cases where you want to filter\na subset of your nodes and edges and convert them directly into PyG ",(0,r.kt)("inlineCode",{parentName:"p"},"HeteroData")," objects (i.e., use PyG's default in-memory storage)\nas you build PyG pipelines using graph databases you store in K\xf9zu.\nIf you are converting a large graph this can be quite slow, and we will be improving this so that such ETL pipelines\nare much faster!"),(0,r.kt)("p",null,"We are excited to hear about your feedback on K\xf9zu's PyG integration features and get more ideas about\nhow else we can help users who are building GNN pipelines. Please reach out to us over ",(0,r.kt)("a",{parentName:"p",href:"https://join.slack.com/t/kuzudb/shared_invite/zt-1w0thj6s7-0bLaU8Sb~4fDMKJ~oejG_g"},"K\xf9zu Slack"),"\nfor your questions and ideas!."),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1-5f0073"},"If you read our ",(0,r.kt)("a",{parentName:"li",href:"https://kuzudb.com/blog/kuzu-0.0.3-release.html#k%C3%B9zu-as-a-pyg-remote-backend"},"v0.0.3 blog post"),",\nwhich had a shorter section about PyG interface, you will notice that we used a much larger batch size there (48000),\nwhich was the size that saturated GPU memory. Although the example there was also on the ",(0,r.kt)("inlineCode",{parentName:"li"},"ogbn-papers100M")," dataset, we used a much smaller model with ~200K parameters\nand sampled subgraphs from 2 degree neighbors of these batches. Now we use a much larger model with 5.6 million parameters and samples from 3-degree neighbors.",(0,r.kt)("a",{parentName:"li",href:"#fnref-1-5f0073",className:"footnote-backref"},"\u21a9")))))}m.isMDXComponent=!0}}]);