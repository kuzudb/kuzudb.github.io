"use strict";(self.webpackChunkkuzu_docs=self.webpackChunkkuzu_docs||[]).push([[157],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>c});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(a),h=r,c=m["".concat(s,".").concat(h)]||m[h]||d[h]||o;return a?n.createElement(c,i(i({ref:t},u),{},{components:a})):n.createElement(c,i({ref:t},u))}));function c(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},3943:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const o={slug:"kuzu-0.0.3-release",authors:["team"],tags:["release"]},i="K\xf9zu 0.0.3 Release",l={permalink:"/docusaurus/blog/kuzu-0.0.3-release",source:"@site/blog/2023-04-06-kuzu-v-0.0.3.md",title:"K\xf9zu 0.0.3 Release",description:"We are happy to release K\xf9zu 0.0.3 today. This release comes with the following new main features and improvements:",date:"2023-04-06T00:00:00.000Z",formattedDate:"April 6, 2023",tags:[{label:"release",permalink:"/docusaurus/blog/tags/release"}],readingTime:10.44,hasTruncateMarker:!0,authors:[{name:"K\xf9zu Team",url:"https://github.com/kuzudb/",key:"team"}],frontMatter:{slug:"kuzu-0.0.3-release",authors:["team"],tags:["release"]},prevItem:{title:"Scaling Pytorch Geometric GNNs With K\xf9zu",permalink:"/docusaurus/blog/kuzu-pyg-remote-backend.html"},nextItem:{title:"Why (Graph) DBMSs Need New Join Algorithms: The Story of Worst-case Optimal Join Algorithms",permalink:"/docusaurus/blog/wcoj"}},s={authorsImageUrls:[void 0]},p=[{value:"K\xf9zu as a PyG Remote Backend",id:"k\xf9zu-as-a-pyg-remote-backend",level:2},{value:"Data Ingestion Improvements",id:"data-ingestion-improvements",level:2},{value:"Query Optimizer Improvements",id:"query-optimizer-improvements",level:2},{value:"New Buffer Manager",id:"new-buffer-manager",level:2},{value:"New Data Types",id:"new-data-types",level:2},{value:"Other System Functionalities",id:"other-system-functionalities",level:2}],u={toc:p},m="wrapper";function d(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"We are happy to release K\xf9zu 0.0.3 today. This release comes with the following new main features and improvements:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#k%C3%B9zu-as-a-pyg-remote-backend"},"K\xf9zu as a Pytorch Geometric (PyG) Remote Backend"),": You can now train PyG GNNs and other models directly using graphs (and node features) stored on K\xf9zu.  See this ",(0,r.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/12fOSqPm1HQTz_m9caRW7E_92vaeD9xq6"},"Colab notebook"),"\nfor a demonstrative example. "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#data-ingestion-improvements"},"Data ingestion from multiple files and numpy files"),": See below for details"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#query-optimizer-improvements"},"Query optimizer improvements"),": See below for details"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#new-buffer-manager"},"New buffer manager"),": A new state-of-art buffer manager based on ",(0,r.kt)("a",{parentName:"li",href:"https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/_my_direct_uploads/vmcache.pdf"},"VMCache"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#new-data-types"},"INT32, INT16, FLOAT, and FIXED LIST data types")," (the latter is particularly suitable to store node features in graph ML applications)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docusaurus/blog/kuzu-0.0.3-release#other-system-functionalities"},"Query timeout mechanism and interrupting queries from CLI"),".")),(0,r.kt)("p",null,"For installing the new version,\nplease visit the ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/#download"},"download section of our website"),"\nand ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/getting-started.html"},"getting started guide")," and the full\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kuzudb/kuzu/releases"},"release notes are here"),". Please visit\nthe ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/getting-started/colab-notebooks"},"Colab Notebooks")," section of our\ndocumentation website to play with our ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/getting-started/colab-notebooks"},"Colab notebooks"),"."),(0,r.kt)("p",null,"Enjoy! Please give us a try, ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/kuzudb/kuzu"},"a Github \u2b50")," and your feedback and feature requests! Also follow\nus on ",(0,r.kt)("a",{parentName:"p",href:"https://twitter.com/kuzudb"},"Twitter"),"!"),(0,r.kt)("h2",{id:"k\xf9zu-as-a-pyg-remote-backend"},"K\xf9zu as a PyG Remote Backend"),(0,r.kt)("p",null,"K\xf9zu now implements PyG's Remote Backend interface. So you can directly\ntrain GNNs using K\xf9zu as your backend storage. Quoting ",(0,r.kt)("a",{parentName:"p",href:"https://pytorch-geometric.readthedocs.io/en/latest/advanced/remote.html"},"PyG documentation's")," description\nof the Remote Backend feature:"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"...","[this feature enables]"," users to train GNNs on graphs far larger than the size of their\nmachine\u2019s available memory. It does so by introducing simple, easy-to-use, and extensible abstractions of a ",(0,r.kt)("inlineCode",{parentName:"p"},"torch_geometric.data.FeatureStore")," and a   ",(0,r.kt)("inlineCode",{parentName:"p"},"torch_geometric.data.GraphStore")," that plug directly into existing familiar PyG interfaces.")),(0,r.kt)("p",null,"With our current release, once you store your graph and features in K\xf9zu,\nPyG's samplers work seamlessly using K\xf9zu's implementation of ",(0,r.kt)("inlineCode",{parentName:"p"},"FeatureStore")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"GraphStore")," interfaces. For example,\nthis enables your existing GNN models to work seamlessly by fetching both subgraph samples and node features\nfrom K\xf9zu instead of PyG's in-memory storage.\nTherefore you can train graphs that do not\nfit into your memory since K\xf9zu, as a DBMS, stores its data on disk. Try this demonstrative ",(0,r.kt)("a",{parentName:"p",href:"https://colab.research.google.com/drive/12fOSqPm1HQTz_m9caRW7E_92vaeD9xq6"},"Colab notebook")," to\nsee an example of how to do this. The current release comes with a limitation that we only truly implement the ",(0,r.kt)("inlineCode",{parentName:"p"},"FeatureStore")," interface.\nInside ",(0,r.kt)("inlineCode",{parentName:"p"},"GraphStore")," we still store the graph topology in memory.\nSo in reality only the features are stored and scanned from disk. We plan to address this limitation later on."),(0,r.kt)("p",null,"Here is also a demonstrative experiment (but certainly not comprehensive study) for the type of training performance\nvs memory usage tradeoff you can expect.\nWe trained a simple 3-layers Graph Convolutional Network (GCN) model on ",(0,r.kt)("a",{parentName:"p",href:"https://ogb.stanford.edu/docs/nodeprop/#ogbn-papers100M"},"ogbn-papers100M")," dataset, which contains about 111 million nodes\nwith 128 dimensional node features and about 1.6 billion edges.\nStoring the graph topology takes around 48GB",(0,r.kt)("sup",{parentName:"p",id:"fnref-1-3b764c"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1-3b764c",className:"footnote-ref"},"1"))," and the features takes 53 GBs. Given our current limitation,\nwe can reduce 53 GB to something much smaller (we will limit it to as low as 10GB).\nWe used a machine with one RTX 4090 GPU with 24 GB of memory, two Xeon Platinum 8175M CPUs, and 384 GB RAM, which\nis enough for PyG's in-memory store to store the entire graph and all features in memory."),(0,r.kt)("p",null,"During training, we use the ",(0,r.kt)("inlineCode",{parentName:"p"},"NeighborLoader")," of PyG with batch size of 48000 and sets the ",(0,r.kt)("inlineCode",{parentName:"p"},"num_neighbors")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"[30] * 2"),", which means at each batch roughly 60 neighbor nodes of 48000 nodes will be sampled from the ",(0,r.kt)("inlineCode",{parentName:"p"},"GraphStore")," and the features of those nodes will be scanned\nfrom K\xf9zu's storage. We picked this sample size because this gives us a peak GPU memory usage of approximately 22 GB, i.e.,\nwe can saturate the GPU memory. We used 16 cores",(0,r.kt)("sup",{parentName:"p",id:"fnref-2-3b764c"},(0,r.kt)("a",{parentName:"sup",href:"#fn-2-3b764c",className:"footnote-ref"},"2"))," during the sampling process. We run each experiment in a Docker instance\nand limit the memory systematically from 110GB, which is enough for PyG to run completely in memory, down to 90, 70, and 60GB.\nAt each memory level we run the same experiment by using K\xf9zu as a Remote Backend, where we\nhave to use about 48GB to store the topology and give the remaining memory to K\xf9zu's buffer manager.\nFor example when the memory is 60GB, we can only give ~10GB to K\xf9zu."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Configuration"),(0,r.kt)("th",{parentName:"tr",align:null},"End to End Time (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"Per Batch Time (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"Time Spent on Training (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"Time Spent on Copying to GPU (s)"),(0,r.kt)("th",{parentName:"tr",align:null},"Docker Memory"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"PyG In-memory"),(0,r.kt)("td",{parentName:"tr",align:null},"140.17"),(0,r.kt)("td",{parentName:"tr",align:null},"1.4"),(0,r.kt)("td",{parentName:"tr",align:null},"6.62"),(0,r.kt)("td",{parentName:"tr",align:null},"31.25"),(0,r.kt)("td",{parentName:"tr",align:null},"110 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=60GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"392.6"),(0,r.kt)("td",{parentName:"tr",align:null},"3.93"),(0,r.kt)("td",{parentName:"tr",align:null},"6.29"),(0,r.kt)("td",{parentName:"tr",align:null},"34.18"),(0,r.kt)("td",{parentName:"tr",align:null},"110 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=40GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"589.0"),(0,r.kt)("td",{parentName:"tr",align:null},"5.89"),(0,r.kt)("td",{parentName:"tr",align:null},"6.8"),(0,r.kt)("td",{parentName:"tr",align:null},"32.6"),(0,r.kt)("td",{parentName:"tr",align:null},"90 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=20GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"1156.1"),(0,r.kt)("td",{parentName:"tr",align:null},"11.5"),(0,r.kt)("td",{parentName:"tr",align:null},"6.0"),(0,r.kt)("td",{parentName:"tr",align:null},"36"),(0,r.kt)("td",{parentName:"tr",align:null},"70 GB")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"K\xf9zu Remote Backend (bm=10GB)"),(0,r.kt)("td",{parentName:"tr",align:null},"1121.92"),(0,r.kt)("td",{parentName:"tr",align:null},"11.21"),(0,r.kt)("td",{parentName:"tr",align:null},"6.88"),(0,r.kt)("td",{parentName:"tr",align:null},"35.03"),(0,r.kt)("td",{parentName:"tr",align:null},"60 GB")))),(0,r.kt)("p",null,"So, when have enough memory, there is about 2.8x slow down (from 1.4s to 3.93s per batch). This\nis the case when K\xf9zu has enough buffer memory (60GB) to store the 53GB of features but we still incur the cost of\nscanning them through K\xf9zu's buffer manager. So no or very little disk I/O happens (except the first time\nthe features are scanned to the buffer manager). Then as we lower the memory, K\xf9zu can hold only part\nof the of node features in its buffer manager, so\nwe force K\xf9zu to do more and more I/O. The per batch time increase to 5.89s at 40GB of buffer manager size,\nthen seems to stabilize around 11s (so around 8.2x slowdown). "),(0,r.kt)("p",null,"The slow down is better if you use smaller batch sizes but for the end to end training time, you\nshould probably still prefer to use larger batch sizes. This is a place where we would need to\ndo more research to see how much performance is on the table with further optimizations."),(0,r.kt)("p",null,"But in summary, if you have\nlarge datasets that don't fit on your current systems' memories and would like to easily train your PyG models\noff of disk (plus get all the usability features of a GDBMS as you prepare your datasets for training),\nthis feature can be very useful for you!"),(0,r.kt)("h2",{id:"data-ingestion-improvements"},"Data Ingestion Improvements"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Ingest from multiple files"),": You can now load data from multiple files of the same type into a node/rel table in two ways:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"file list"),": ",(0,r.kt)("inlineCode",{parentName:"li"},'["vPerson0.csv", "vPerson1.csv", "vPerson2.csv"]')),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"glob pattern matching"),": Similar to Linux ",(0,r.kt)("a",{parentName:"li",href:"https://man7.org/linux/man-pages/man7/glob.7.html"},"Glob"),", this will load files that matches the glob pattern.")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Ingest from npy files"),": We start exploring how to enable data ingesting in column by column fashion. Consider a ",(0,r.kt)("inlineCode",{parentName:"p"},"Paper")," table defined in the following DDL."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"CREATE NODE TABLE Paper(id INT64, feat FLOAT[768], year INT64, label DOUBLE, PRIMARY KEY(id));\n")),(0,r.kt)("p",null,'Suppose your raw data is stored in npy formats where each column is represented as a numpy array on disk:\n"node_id.npy", "node_feat_f32.npy", "node_year.npy", "node_label.npy".\nYou can now directly copy from npy files where each file is loaded to a column in ',(0,r.kt)("inlineCode",{parentName:"p"},"Paper")," table as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'COPY Paper FROM ("node_id.npy", "node_feat_f32.npy", "node_year.npy", "node_label.npy") BY COLUMN;\n')),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Reduce memory consumption when ingesting data into node tables:"),"\nThis release further optimizes the memory consumption during data ingestion of node tables.\nWe no longer keep the whole node table in memory before flushing it to disk as a whole. Instead, we process a chunk of a file\nand flush its corresponding pages, so incur only the memory cost of ingesting a chunk (or as many chunks as there are threads running).\nThis greatly reduces memory usage when the node table is very large."),(0,r.kt)("h2",{id:"query-optimizer-improvements"},"Query Optimizer Improvements"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Projection push down for sink operator"),":\nWe now push down projections down to the first sink operator\nabove the last point in a query plan they are needed.\nConsider the following query"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"MATCH (a:person) WHERE a.age > 35 RETURN a.salary AS s ORDER BY s;\n")),(0,r.kt)("p",null,"This query's (simplified) plan is:  ",(0,r.kt)("inlineCode",{parentName:"p"},"Scan->Filter->OrderBY->ResultCollector"),", where both\n",(0,r.kt)("inlineCode",{parentName:"p"},"ORDER BY")," and the final ",(0,r.kt)("inlineCode",{parentName:"p"},"ResultCollector")," are sink operators.\n",(0,r.kt)("inlineCode",{parentName:"p"},"ResultCollector")," is where we accumulate the expressions in the ",(0,r.kt)("inlineCode",{parentName:"p"},"RETURN")," clause.\nThis is simplified because ",(0,r.kt)("inlineCode",{parentName:"p"},"ORDER BY")," actually consists of several physical operators.\nBoth column ",(0,r.kt)("inlineCode",{parentName:"p"},"age")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"salary")," are scanned initially but only ",(0,r.kt)("inlineCode",{parentName:"p"},"salary")," is needed in ",(0,r.kt)("inlineCode",{parentName:"p"},"ResultCollector"),".\n",(0,r.kt)("inlineCode",{parentName:"p"},"age"),", which is needed by ",(0,r.kt)("inlineCode",{parentName:"p"},"Filter")," is projected out in the ",(0,r.kt)("inlineCode",{parentName:"p"},"ResultCollector"),". We now push the projection of ",(0,r.kt)("inlineCode",{parentName:"p"},"age"),"\nto ",(0,r.kt)("inlineCode",{parentName:"p"},"ORDER BY"),", so ",(0,r.kt)("inlineCode",{parentName:"p"},"ORDER BY")," does not have to materialize it."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Other optimizations:")," We implemented several other optimizations, such as we reorder the filter expressions so equality conditions\nare evaluated first, several improvements to cardinality estimator, and improved sideway information passing for joins. For the latter,\nin our core join operator, which we called  ASP-Joins in our ",(0,r.kt)("a",{parentName:"p",href:"https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf"},"CIDR paper"),", we would blindly\nperform sideways information passing (sip) from build to probe (or vice versa;\nsee ",(0,r.kt)("a",{parentName:"p",href:"https://www.cidrdb.org/cidr2023/papers/p48-jin.pdf"},"our paper")," for details). Sometimes if there is no\nfilters on the probe and build sides, this is just an overhead as it won't decrease the amount of scans on either side.\nIn cases where we think sip won't help reduce scans, we do vanilla Hash Joins now."),(0,r.kt)("h2",{id:"new-buffer-manager"},"New Buffer Manager"),(0,r.kt)("p",null,"Before this release, we had two internal buffer pools with 2 different frame sizes of 4KB and 256KB,\nso operators could only grab buffers of these two sizes. Plus when you loaded your DB and wanted to allocate\nsay 10GB buffer pool, we manually gave a fixed percentage to 4KB pool and the rest to 256KB pool.\nThis didn't give any flexibility for storing large objects and complicated code to manage\nbuffers when operators needed them.  Terrible design;\njust don't do this!"),(0,r.kt)("p",null,"We bit the bullet and decided to read the literature and pick a state-of-art buffer manager design that is\nalso practical. We switched to the mmap-based approach described in VMCache design from ",(0,r.kt)("a",{parentName:"p",href:"https://www.cs.cit.tum.de/fileadmin/w00cfj/dis/_my_direct_uploads/vmcache.pdf"},"this recent paper")," by Leis et al..\nThis is a very nice design\nand makes it very easy to support multiple buffer sizes very easily and only uses hardware locks (we used\nsoftware locks in our previous buffer manager). It also supports using optimistic reading,\nwhich we verified improves our query performance a lot."),(0,r.kt)("h2",{id:"new-data-types"},"New Data Types"),(0,r.kt)("p",null,"We now support several additional data types that were missing."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("a",{parentName:"strong",href:"https://kuzudb.com/docs/cypher/data-types/list.html"},"FIXED-LIST")," data type:")," This is important if you're doing graph ML and storing node features\nin K\xf9zu. It is the efficient way to store fixed-length vectors. Here's the summary of how\nto declare a node or rel property in your schemas to use the fixed-list data type."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Data Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"DDL definition"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"FIXED-LIST"),(0,r.kt)("td",{parentName:"tr",align:null},"a list of fixed number of values of the same numerical type"),(0,r.kt)("td",{parentName:"tr",align:null},"INT64","[8]")))),(0,r.kt)("p",null,"When possible use FIXED LIST instead of regular ",(0,r.kt)("a",{parentName:"p",href:"https://kuzudb.com/docs/cypher/data-types/list.html"},"VAR-LIST")," data type\nfor cases when you know the size of your lists/vectors. It's much more efficient."),(0,r.kt)("p",null,"Note that FIXED-LIST is an experimental feature. Currently only bulk loading (e.g. ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," statement) and reading is supported."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"INT32, INT16, FLOAT data types:")," The release also comes with support for the following data types:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Data Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Size"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"INT32"),(0,r.kt)("td",{parentName:"tr",align:null},"4 bytes"),(0,r.kt)("td",{parentName:"tr",align:null},"signed four-byte integer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"INT16"),(0,r.kt)("td",{parentName:"tr",align:null},"2 bytes"),(0,r.kt)("td",{parentName:"tr",align:null},"signed two-byte integer")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"FLOAT"),(0,r.kt)("td",{parentName:"tr",align:null},"4 bytes"),(0,r.kt)("td",{parentName:"tr",align:null},"single precision floating-point number")))),(0,r.kt)("p",null,"For our next release, our focus on data types will be on complex ones, STRUCT and MAP. So stay tuned for those!"),(0,r.kt)("h2",{id:"other-system-functionalities"},"Other System Functionalities"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Query timeout"),": We will now automatically stop any query that exceeds a specified timeout value (if one exists).\nThe default query timeout value is set to -1, which signifies that the query timeout feature is initially disabled.\nYou can activate the query timeout by configuring a positive timeout value through:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("ol",{parentName:"li"},(0,r.kt)("li",{parentName:"ol"},"C++ API: ",(0,r.kt)("inlineCode",{parentName:"li"},"Connection::setQueryTimeOut(uint64_t timeoutInMS)")))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("ol",{parentName:"li",start:2},(0,r.kt)("li",{parentName:"ol"},"CLI: ",(0,r.kt)("inlineCode",{parentName:"li"},":timeout [timeoutValue]"))))),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Interrupt:")," You can also interrupt your queries and can stop your long running queries manually. There\nare two ways to do this:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"C++ API: ",(0,r.kt)("inlineCode",{parentName:"li"},"Connection::interrupt()"),": interrupt all running queries within the current connection."),(0,r.kt)("li",{parentName:"ul"},"CLI: interrupt through ",(0,r.kt)("inlineCode",{parentName:"li"},"CTRL + C"))),(0,r.kt)("p",null,"Note: The Interruption and Query Timeout features are not applicable to ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," commands in this release."),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1-3b764c"},"Internally, PyG coverts the edge list to CSC format for sampling, which duplicates the graph structures in memory. When you download the graph topology it actually takes about 24GB.",(0,r.kt)("a",{parentName:"li",href:"#fnref-1-3b764c",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-2-3b764c"},"We set ",(0,r.kt)("inlineCode",{parentName:"li"},"num_workers")," to 16 when running the PyG in-memory setup. Since K\xf9zu does not currently work with multiple workers in Python, we limit ",(0,r.kt)("inlineCode",{parentName:"li"},"num_workers")," to 1 when sampling from K\xf9zu but internally K\xf9zu scans in parallel with 16 threads.",(0,r.kt)("a",{parentName:"li",href:"#fnref-2-3b764c",className:"footnote-backref"},"\u21a9")))))}d.isMDXComponent=!0}}]);